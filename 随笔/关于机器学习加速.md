# 关于机器学习的加速

从通用芯片（CPU）到图形处理芯片（CUDA）到可定制芯片（FPGA）再到专用芯片（ASIC），机器学习对实时性处理的要求越来越高，处理机器学习算法的芯片类型也在不断的变化。
那么究竟未来发展的方向如何？

机器学习算法中运算量最大的矩阵运算是一种矢量（Vector）运算，而CPU对于矢量运算只能说是部分支持。这时候，GPU进入了机器学习研究者的视野。GPU原本的目的是图像渲染，因此使用完美支持矢量运算的SIMD（单指令流多数据流，single instruction multiple data）架构，而这个架构正好能用在机器学习算法上。
GPU运行机器学习算法比CPU快很多，但是毕竟不是为机器学习而设计的。有人就要问，如果做一块完全为机器学习设计的运算单元，会不会比GPU更有效率？不过，要真的做一块机器学习专用芯片（ASIC）需要极大的决心，首先为了性能必须使用最好的半导体制造工艺，而现在用最新的工艺制造芯片一次性成本就要几百万美元，非常贵。就算你有钱，你还需要拉一支队伍从头开始设计，设计时间往往要到一年以上，time to market时间太长，风险很大，所以除了Google之外很少有人敢做ASIC。这时候，FPGA就吸引了大家的注意力。
FPGA全称“可编辑门阵列”(Field Programmable Gate Array)，其基本原理是在FPGA芯片内集成大量的数字电路基本门电路以及存储器，而用户可以通过烧入FPGA配置文件来来定义这些门电路以及存储器之间的连线。这种烧入不是一次性的，即用户今天可以把FPGA配置成一个微控制器MCU，明天可以编辑配置文件把同一个FPGA配置成一个音频编解码器。所以说在FPGA可以快速实现为机器学习算法开发的处理器架构，而且成本很低（一块FPGA开发板大约售价1000美金，比真的制造芯片便宜太多）。ASIC是一锤子买卖，设计出来要是发现哪里不对基本就没机会改了，但是FPGA可以通过重新配置来不停地试错知道获得最佳方案，所以用FPGA开发的风险也远远小于ASIC。

微软的HoloLens就是用了定制的ASIC芯片来做图像处理，果然财大气粗。
不过随着机器学习的普及，对于其加速的普及也是一个推动，应该很少会用到ASIC这种大杀器，而是采用比较通用的，性能足够的方式，在传统消费领域应该还是CUDA为主，大型行业领域FPGA为主，专业细分领域ASIC为主。

2017年2月28日16:54:36
补充上述观点，随着处理业务的发展，芯片的专有化和特化也是一个趋势，为了应对专用场景研发专用芯片会是以后大型企业的重点，因为随着大数据时代的发展，单个计算成本的下降无法保证大数量计算成本的下降，所以这种计算芯片的特化也是无法避免的。
如果多种芯片同时工作，那么芯片之间的信息交换效率也需要进行提升，数据的传输瓶颈会制约和刺激大规模计算的发展，存储的速度也会得到要求，例如目前的nvme，量变到质变是一个趋势。
同时，随着摩尔定律失效，硅基电路的物理限制愈发制约芯片发展，面向量子计算芯片的研究也就非常迫切，同时为了应对这个问题，多核心的叠加也是无法避免的，这会对当前软件设计提出新的要求，对于多核心的利用方法需要非常科学合理才行，这样才能提升计算能力。

能量的限制、计算能力的限制和信息传递速度的限制，这三大因素将会是未来科技发展的主要制约因素，任何一方面的突破将会给整个世界带来非常巨大的影响，才能让人们进入更新的一层领域来认识这个世界。
工业革命也是逐步发生的，这个过程中就要没一项进展自由发展，看看能带来什么新的理解和改变，量变到质变的积累过程。

这里有个体会：需要在自己的头脑中建模来完善的分析整个系统的瓶颈所在，各个环节之间的关联性和每一个环节的核心作用与受限关系，

> -  参考文档：
[芯片架构换血！如何评价微软在数据中心使用FPGA？](http://www.msra.cn/zh-cn/news/blogs/2017/01/fpga-20170111.aspx)
[服务器端人工智能，FPGA和GPU到底谁更强？](http://www.jiqizhixin.com/article/1894)